<h1>Interpret Files</h1>
<p>This GitHub project allows you to interpret any files you upload, including .BIN, .WAV, .PAK, .PNG, and more! However, beware that the visuals and the audio will be processed for you to look at/listen to, and the visuals and audio will be very erratic and unpredictable (depending on what file you're interpreting). <a style="color: red">MAKE SURE TO DISABLE THE VISUAL INTERPRETATION IF YOU ARE SENSITIVE TO FLASHING LIGHTS!</a> You can see the privacy policy <a href="privacy.html">here</a>.</p>
<audio id="audioPlay" controls></audio>
<input type="file" id="fileInput" multiple>
<hr>
<select id="visualType"><option value="a">Waterfall</option><option value="w">Waveform</option><option value="d">Sketch</option></select>
<p id="description"></p>
Display visualizer? <input id="shouldDisplay" type="checkbox" checked>
<canvas width="256" height="256" id="visualizer"></canvas><hr>
<button id="startBtn">Start Recording</button> <button id="stopBtn">Stop Recording</button><br><video id="preview" controls></video>
<script>const audioPlayer = document.getElementById("audioPlay"), wa = document.getElementById("visualizer"), ddd = document.getElementById("shouldDisplay");
let s = wa.getContext("2d");
const options = [...document.querySelectorAll("option")], choice = document.getElementById("visualType"), desc = document.getElementById("description");
class AudioExporter { // This class is meant for the original programmer's development, not for public use; implement error handling, and handle other bits if invalid types are expected 
	constructor(audioData, sampleRate, channels, bits) {
		this.audioData = audioData;
		this.sampleRate = sampleRate;
		this.channels = channels;
		this.bits = bits;
	}
	convertToWav(floatPoint = false) {
		const numChannels = this.channels, len = this.audioData.length;
		if (floatPoint) this.bits = 32;
		const len2 = len * (this.bits / 8);
		const buffer = new ArrayBuffer(44 + len2);
		const view = new DataView(buffer);
		this.writeString(view, 0, 'RIFF');
		view.setUint32(4, 36 + len2, true);
		this.writeString(view, 8, 'WAVE');
		this.writeString(view, 12, 'fmt ');
		view.setUint32(16, 16, true);
		view.setUint16(20, floatPoint ? 3 : 1, true); // The type of PCM (1 = integer-based, 3 = IEEE 754 floating-point-based)
		view.setUint16(22, numChannels, true);
		view.setUint32(24, this.sampleRate, true);
		view.setUint32(28, this.sampleRate * this.channels * (this.bits / 8), true);
		view.setUint16(32, this.channels * (this.bits / 8), true);
		view.setUint16(34, this.bits, true);
		this.writeString(view, 36, 'data');
		view.setUint32(40, len2, true);
		let offset = 44, pointer = this.audioData;
		if (floatPoint) {
			for (let i = 0; i < len; i++) {
				view.setFloat32(offset, pointer[i], true);
				offset += 4;
			}
		} else {if (this.bits === 8) {
			for (let i = 0; i < len; i++) {
				view.setUint8(offset, pointer[i], true);
				offset++;
			}
		} else {
			for (let i = 0; i < len; i++) {
				view.setInt16(offset, pointer[i], true);
				offset += 2;
			}
		}}
		return new Blob([view.buffer], { type: 'audio/wav' });
	}
	writeString(view, offset, string) {
		for (let i = 0; i < string.length; i++) {
			view.setUint8(offset + i, string.charCodeAt(i));
		}
	}
};

ddd.oninput = function() {
	wa.style.display = ddd.checked ? "" : "none";
}

let audioData, exporter, bitmax, divis, shouldRedraw, point, prevDestination = 0;
setInterval(() => s = wa.getContext("2d"), 2000);
function update(forceRedraw) {
	//if (ddd.checked || !audioData || audioData.length === 0) return;
	// //if (!(shouldRedraw || forceRedraw || audioPlayer.currentTime !== point)) return;
	if (!exporter) return;
	if (choice.value === "a") {
		const destination = Math.round(audioPlayer.currentTime * exporter.sampleRate * exporter.channels / 64) * 64;
		const size = 256;

		const imgData = s.getImageData(0, 0, 256, 256);
		const data = imgData.data;
		console.log(data);

		for (let y = 0; y < size; y++) {
			for (let x = 0; x < size; x++) {
				const off = x * 3 + y * size;
				const z = Math.floor(destination + off);
				const idx = (y * size + x) * 4; // RGBA in ImageData

				if (z >= audioData.length) {
					data[idx] = 0;
					data[idx + 1] = 0;
					data[idx + 2] = 0;
					data[idx + 3] = 255;
				} else {
					const shou = prevDestination + off;
					if (
						audioData[shou] !== audioData[z] ||
						audioData[shou + 1] !== audioData[z + 1] ||
						audioData[shou + 2] !== audioData[z + 2]
					) {
						data[idx] = (audioData[z] + bitmax) / divis;
						data[idx + 1] = (audioData[z + 1] + bitmax) / divis;
						data[idx + 2] = (audioData[z + 2] + bitmax) / divis;
						data[idx + 3] = 255;
					}
				}
			}
		}

		s.putImageData(imgData, 0, 0);
		prevDestination = destination;
	} else if (choice.value === "w") {
			s.lineWidth = 2;
			s.fillStyle = "rgb(0,0,0)";
			s.fillRect(0, 0, 512, 256);
			const destination = Math.round(audioPlayer.currentTime * exporter.sampleRate * exporter.channels);
			s.beginPath();
			s.moveTo(0, 128);
			for (let x = 0; x < 512; x++) {
				s.lineTo(x, (audioData[destination + x] + bitmax) / divis);
			}
			s.strokeStyle = "rgb(255,255,255)";
			s.stroke();
		} else {
			s.lineWidth = 2;
			s.fillStyle = "rgb(0,0,0)";
			s.fillRect(0, 0, 256, 256);
			const destination = Math.round(audioPlayer.currentTime * exporter.sampleRate * exporter.channels);
			s.beginPath();
			for (let x = 0; x < 512; x += 2) {
				let rrd = destination + x;
				s.lineTo((audioData[rrd] + bitmax) / divis, (audioData[rrd + 1] + bitmax) / divis);
			}
			s.strokeStyle = "rgb(255,255,255)";
			s.stroke();
		}
	shouldRedraw = !audioPlayer.paused;
	point = audioPlayer.currentTime;
	/*if (!forceRedraw) */ requestAnimationFrame(update);
}
update();
document.getElementById("fileInput").addEventListener('change', (event) => {
	if (event.target.files.length === 1) {
		const file = event.target.files[0];
		const reader = new FileReader();
		// When the file is successfully read (why does this sound like a meme?)
		reader.onload = function(e) {
			const arrayBuffer = e.target.result, mode = parseFloat(prompt("What mode should be used for the interpretation process? 0 = basic interpretation: file data is interpeted as audio in the way it was organized. 1 = flipped interpretation: file data is interpreted identically to the basic mode, except the decoding order is flipped for the 16-bit option.")) >>> 0, bit = parseFloat(prompt("How many bits should each sample accept? (Default is 8 bits each)") || "8") >>> 0;
			if (bit !== 8 && bit !== 16) {
				alert("The amount of bits should be either 8 or 16. Defaulting to 8 bits instead.");
				bit = 8;
			}
			const ne = Math.pow(2, bit) / 2;
			if (bit === 8) {
				if (mode === 0) {
					audioData = new Int8Array(arrayBuffer); // LOOK, MOM!! I OPTIMIZED MY DATA INTERPRETATION STEPS!!! ISNT THAT COOL????
				} else if (mode === 1) {
					audioData = new Int8Array(audioData.length);
					const arr = new Uint8Array(audioData);
					for (let i = 0; i < audioData.length; i++) {
						audioData[i] = arr[i] - ne;
					}
				}
			} else {
				if (mode === 0) {
					if (arrayBuffer.byteLength % 2 === 0) {
						audioData = new Int16Array(arrayBuffer); // I wonder what this sounds like when I interpret 16-bit WAV sounds... oh, they sound the same.
					} else {
						// OH, NOES! WE RAN INTO A PROBLEM!
						const uint8 = new Uint8Array(Math.ceil(arrayBuffer.byteLength / 2) * 2);
						uint8.set(new Uint8Array(arrayBuffer));
						audioData = new Int16Array(uint8.buffer);
					}
				} else if (mode === 1) {
					audioData = new Int16Array(arrayBuffer.byteLength / 2);
					const arr = new Uint8Array(arrayBuffer);
					const len = Math.ceil(arrayBuffer.byteLength / 4) * 2, len2 = Math.ceil(arrayBuffer.byteLength / 4) * 2;
					for (let i = 0; i < len2; i++) {
						audioData[i] = (256 * arr[i << 1] + arr[(i << 1) + 1]) - ne;
					}
					for (let i = 0; i < len; i++) {
						audioData[i] = 256 * arr[i << 1] - ne;
					}
				}
			}
			exporter = new AudioExporter(audioData, Number(prompt("Enter in the sample rate of the audio (48000 will be the default if invalid input, or input not received).") || 48000), Number(prompt("Enter in the channels of the audio (1 will be the default if invalid input, or input not received).") || 1), bit);
			if (audioPlayer.src) URL.revokeObjectURL(audioPlayer.src);
			audioPlayer.src = URL.createObjectURL(exporter.convertToWav());
			alert("WARNING: THE RESULT OF THE AUDIO MAY CONTAIN VERY HIGH FREQUENCIES FOR DATA THAT MAY VARY A LOT!");
			bitmax = exporter.bits === 8 ? 128 : 32768;
			divis = exporter.bits === 8 ? 1 : 256;
			update(true);
		};
		// Read the file as an ArrayBuffer
		alert("The file has been uploaded to the tool (not sent to the internet). Pressing OK may begin the reading process."), reader.readAsArrayBuffer(file);
	}
});
s.strokeStyle = "rgb(255,255,255)";
s.lineWidth = 1;
s.lineCap = 'round';
s.lineJoin = 'bevel';
const map = ["Uses 4x4 RGB pixels to directly represent the audio data, where all of the pixels scroll up until the audio ends.", "Uses a white line on a black background to represent the audio data's sample values.", "Uses many white lines attached to each other to represent a cool waveform."];
for (let i = 0; i < options.length; i++) {
	options[i].onmouseover = function() {
		description.textContent = map[i];
	}
	options[i].onmouseout = function() {
		description.textContent = "";
	}
}
choice.oninput = function() {
	if (choice.value === "a" || choice.value === "d") {
		wa.width = 256;
	} else if (choice.value === "w") {
		wa.width = 512;
	}
	wa.height = 256;
}

const canvas = wa;
const ctx = canvas.getContext("2d");
const audioEl = audioPlayer;

let recorder;
let recordedChunks = [];

const canvasStream = canvas.captureStream(60); // video stream
const audioCtx = new AudioContext();

// Route the <audio> element through Web Audio
const sourceNode = audioCtx.createMediaElementSource(audioEl);
const destinationNode = audioCtx.createMediaStreamDestination();

// Connect <audio> to destination
sourceNode.connect(destinationNode);
sourceNode.connect(audioCtx.destination); // so you can still hear it

const audioStream = destinationNode.stream;

// Combine canvas video stream and audio stream
const combinedStream = new MediaStream([
  ...canvasStream.getVideoTracks(),
  ...audioStream.getAudioTracks()
]);

document.getElementById("startBtn").onclick = () => {
  recordedChunks = [];

  recorder = new MediaRecorder(combinedStream, { mimeType: "video/webm" });

  recorder.ondataavailable = e => {
	if (e.data.size > 0) recordedChunks.push(e.data);
  };

  recorder.onstop = () => {
	const blob = new Blob(recordedChunks, { type: "video/webm" });
	const url = URL.createObjectURL(blob);
	document.getElementById("preview").src = url;
	audioEl.controls = true;
	const link = document.createElement("a");
	link.href = url;
	link.download = "Recorded Output.webm";
	link.click();
	audioEl.onended = () => {};
  };

  audioEl.play();
  recorder.start();
  audioEl.controls = false;
  audioEl.onended = () => recorder.stop();
};
let sea = true;
document.onclick = function() {
	if (sea) {
		audioCtx.resume();
		sea = false;
	}
} // required in some browsers
document.getElementById("stopBtn").onclick = () => {
  recorder.stop();
};
</script>
